{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tutorial: Analyzing Weather Data using Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[**GETTING STARTED:**](#Data-Scientist-Workbench)  \n",
    "\n",
    "&nbsp;&nbsp;[**Data Scientist Workbench**](#Data-Scientist-Workbench)   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[IPython Notebook](#IPython-Notebook)  \n",
    "\n",
    "&nbsp;&nbsp;[**How to use Data Scientist Workbench**](#How-to-use-Data-Scientist-Workbench)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[How do I upload a CSV file onto Data Scientist Workbench?](#How-do-I-upload-a-CSV-file-onto-Data-Scientist-Workbench?)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[How do I change the name of my uploaded CSV file?](#How-do-I-change-the-name-of-my-uploaded-CSV-file?)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[How do I import a file from the Internet - an external URL?](#How-do-I-import-a-file-from-the-Internet---an-external-URL?)  \n",
    "\n",
    "\n",
    "&nbsp;&nbsp;[**Big Data University**](#Big-Data-University)\n",
    "\n",
    "&nbsp;&nbsp;[**Apache Spark**](#Apache-Spark)  \n",
    "\n",
    "&nbsp;&nbsp;[**SparkSQL Dataframes**](#SparkSQL-Dataframes)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[What is Spark SQL and what is a DataFrame?](#What-is-Spark-SQL-and-what-is-a-DataFrame?)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[More information on Spark SQL and DataFrames](#More-information-on-Spark-SQL-and-DataFrames)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [**TUTORIAL: Data Preparation with Spark using Weather Data**](#Tutorial:-Data-Analysis-with-Spark-using-Weather-Data)  \n",
    "\n",
    "&nbsp;&nbsp;[**The Data:**](#Data-source)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Understanding the Weather Data](#Understanding-the-Weather-Data)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Data source](#Data-source)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Get the data](#Get-the-data)  \n",
    "\n",
    "&nbsp;&nbsp;[**Initial Steps:**](#Import-modules)    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Import modules](#Import-modules)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Understanding the API](#Understanding-the-API)\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Set the API key](#Set-the-API-key)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Import weather stations dataset](#Import-weather-stations-dataset)  \n",
    "\n",
    "&nbsp;&nbsp;[**Creating Functions to Process the Data**](#Creating-Functions-to-Process-the-Data)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[[Function] getDates](#[Function]-getDates)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[[Function] saveUrlRDD](#[Function]-saveUrlRDD)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[[Function] preprocess](#[Function]-preprocess)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[[Function] save_csv](#[Function]-save_csv)  \n",
    "\n",
    "\n",
    "[**Running the scripts**](#Running-the-scripts)   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1. Select start and end date](#1.-Select-start-and-end-date)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2. Run the scripts](#2.-Run-the-scripts)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3. Quality-check the results](#3.-Quality-check-the-results)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4. Check to make sure the output .csv file exists](#4.-Check-to-make-sure-the-output-.csv-file-exists)  \n",
    "\n",
    "[**Contact the Notebook Authors**](#Contact-the-Notebook-Authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scientist Workbench\n",
    "The Data Scientist Workbench is built around the IPython Notebook and offers the following capabilities:\n",
    "\n",
    "1. Runs in the cloud\n",
    "2. Allows users to tag and organize notebooks and data\n",
    "3. Allows users to search notebooks\n",
    "4. Enables users to share and import notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPython Notebook\n",
    "\n",
    "[IPython Notebook](http://ipython.org) is a web-based environment for interactive computing.  IPython Notebook enables you to write and execute code within a \"notebook\" in your web browser.  You enter code into an input cell, and when you run the cell, the notebook executes the code and prints any output to an output cell.  You can change the code in an input cell and re-run the cell as often as you like.  In this way, the notebook follows a [Read Evaluate Print Loop](http://en.wikipedia.org/wiki/Read–eval–print_loop) paradigm.\n",
    "\n",
    "But that's not all.  The notebook also supports rendering markup cells (like this one) inline, so you can embed text, [markdown](http://daringfireball.net/projects/markdown/), HTML, images, videos, and even interactive widgets, all within a notebook.\n",
    "\n",
    "The flow of a notebook is top to bottom, and you can create as many cells as you desire.  The interactive nature and the ability to render text and media makes IPython Notebook a powerful environment for working with data, performing analyses, and documenting results.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** In these tutorials, we assume you have a basic familiarity with the [Python Programming Language](https://docs.python.org/3/) and the [IPython Notebook](http://nbviewer.ipython.org/github/ipython/ipython/blob/2.x/examples/Notebook/Index.ipynb). If you need more background information, we recommend these popular websites and notebooks:\n",
    "\n",
    "<ul>\n",
    "<li>[Learn Python the Hard Way](http://learnpythonthehardway.org/book/)\n",
    "<li>[Introduction to the IPython Notebook](http://nbviewer.ipython.org/github/ipython/ipython/blob/master/examples/Notebook/Index.ipynb)\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use Data Scientist Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do I upload a CSV file onto Data Scientist Workbench?\n",
    "From your local folder on your computer, **drag the file into this page.**  \n",
    "When the upload is complete, you will see your file under **\"Recent Data\"** in the sidebar to the right!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do I change the name of my uploaded CSV file?\n",
    "\n",
    "To change the name:  \n",
    "1. Click on the \"`>`\" button to the left of your file in the **\"Recent Data\"** sidebar on the right.\n",
    "2. Click on _Rename_ to change the filename, and press _Enter_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do I import a file from the Internet - an external URL?\n",
    "You can import files from external URLs to your Data Scientist Workbench. You simply copy the URL into the search bar the top right corner of the page, press _Enter_, and the file will be automatically downloaded to the **\"Recent Data\"** sidebar. Note that the URL file must meet the following requirements:\n",
    "\n",
    "1. Supports HTTP or HTTPS protocol\n",
    "1. Supported file formats:\n",
    "    * Plain text files\n",
    "    * CSV format file\n",
    "    * JSON format files (including notebooks supported * .ipynb format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data University\n",
    "**Big Data University** ([www.BigDataUniversity.com](http://bigdatauniversity.com)) is an educational resource where all of the courses are free, video-based, and you can learn at your own pace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Apache Spark [http://spark.apache.org/](http://spark.apache.org/)? Learn more about Apache Spark through [**Big Data University**](http://bigdatauniversity.com):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**Spark Fundamentals I**](http://bigdatauniversity.com/bdu-wp/bdu-course/spark-fundamentals/)\n",
    "    - Describe what Spark is all about know why you would want to use Spark \n",
    "    - Use Resilient Distributed Datasets operations \n",
    "    - Use Scala, Java, or Python to create and run a Spark application \n",
    "    - Create applications using Spark SQL, MLlib, Spark Streaming, and GraphX \n",
    "    - Configure, monitor and tune Spark  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**Spark Fundamentals II**](http://bigdatauniversity.com/bdu-wp/bdu-course/spark-fundamentals-ii/) \n",
    "    - Apache Spark architecture overview \n",
    "    - Understanding input, partitioning, and parallelization \n",
    "    - Optimizations for efficiently operating on and joining multiple datasets \n",
    "    - Understanding how Spark instructions are translated into jobs and what causes multiple stages within a job \n",
    "    - Efficiently using Spark’s memory caching for iterative processing \n",
    "    - Developing, testing, and debugging Spark applications using SBT, Eclipse, and IntelliJ - \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###What is Spark SQL and what is a DataFrame?\n",
    "Spark SQL is Apache Spark's module for working with structured data. It provides a programming abstraction called DataFrames and can also act as distributed SQL query engine.   \n",
    "A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in Python, but with richer optimizations under the hood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###More information on Spark SQL and DataFrames\n",
    "For more information, see the official guide here: [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tutorial: Data Preparation with Spark using Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Understanding the Weather Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###`stations.txt`:\n",
    "A table containing information on 97 weather stations at international airports throughout the United States.\n",
    "- **ID**: is the station identification code. Please see 'ghcnd-stations.txt' at ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ for a complete list of stations and their metadata.\n",
    "- **LATITUDE**: is latitude of the station (in decimal degrees)\n",
    "- **LONGITUDE**: is the longitude of the station (in decimal degrees)\n",
    "- **ELEVATION**: is the elevation of the station (in meters, missing = -999.9)\n",
    "- **NAME**: is the name of the station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data source\n",
    "\n",
    "Check API documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the command below to download `stations.csv` directly into Data Scientist Workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget -O /resources/stations.txt https://ibm.box.com/shared/static/pplb4jcylm728lpen0jw5yfr5myhc78j.txt\n",
    "print 'You should now see stations.csv under \"Recent data\" in your sidebar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import shutil\n",
    "import os\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import HiveContext\n",
    "from datetime import timedelta, date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we send API GET requests to api.weather.com, in the format:\n",
    "\n",
    "> `http://api.weather.com/v1/geocode/MY_LATITUDE/MY_LONGITUDE/observations/historical.json?apiKey=MY_APIKEY&units=m&startDate=YYYYMMDD&endDate=YYYYMMDD`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the link that there are parameters that need to be set:\n",
    "1. **MY_LATITUDE & MY_LONGITUDE:** The latitude and longitude of the weather station. We have prepared a list of weather stations, as described in the next section\n",
    "1. **MY_APIKEY:** The API key provided to you for api.weather.com\n",
    "1. **units=m:** You can change m, which is currently set to Metric units:  \n",
    "    - For en-US or en, the default units of measure code  is English/Imperial. The units code is “e” .\n",
    "    - For en-gb, the default units of measure is Hybrid-UK. The units code is  “h”.\n",
    "    - For everything else, the default units of measure is  Metric. The units code is “m”.\n",
    "1. **startDate and endDate:** The range that you want to retrieve data for. The maximum range between the two dates is 31 days.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apiKey = 'MY_APIKEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Import weather stations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stRDD = sc.textFile('/resources/stations.txt') #international airports only\n",
    "stRDD = stRDD.map(lambda x: (x[0:13].strip(), #ID\n",
    "                           x[12:20].strip(), #LATITUDE\n",
    "                           x[20:30].strip(), #LONGITUDE\n",
    "                           x[41:71].strip() #NAME\n",
    "                          ))\n",
    "\n",
    "stRDD.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check contents of stations.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'USW00003017', u'39.8328', u'-104.6575', u'DENVER INTL AP'),\n",
       " (u'USW00003102', u'34.0561', u'-117.6003', u'ONTARIO INTL AP'),\n",
       " (u'USW00003196', u'31.4208', u'-110.8458', u'NOGALES INTL AP'),\n",
       " (u'USW00003822', u'32.1300', u'-81.2100', u'SAVANNAH INTL AP'),\n",
       " (u'USW00003856', u'34.6439', u'-86.7861', u'HUNTSVILLE INTL AP')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many stations are there in stations.csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Functions to Process the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Function] getDates \n",
    "\n",
    "This function returns a list of tuples containing the start and end date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDates(start_date, end_date, delta):\n",
    "    '''\n",
    "    start_date: starting date in format YYYYMMDD\n",
    "    end_date: end date in format YYYYMMDD\n",
    "    delta: an integer; the interval between start_date and end_date\n",
    "    '''\n",
    "    from datetime import timedelta, date\n",
    "    delta = delta -1\n",
    "    #Convert dates to date format\n",
    "    s = datetime.date(int(str(start_date)[:4]),int(str(start_date)[4:6]),int(str(start_date)[6:]))\n",
    "    e = datetime.date(int(str(end_date)[:4]),int(str(end_date)[4:6]),int(str(end_date)[6:]))\n",
    "\n",
    "    #Set delta\n",
    "    if (int(delta) > 30 or int(delta) < 1):\n",
    "        raise ValueError('Error: delta out of range. 1 <= delta <= 30')\n",
    "    else:\n",
    "        delta = timedelta(days=int(delta))\n",
    "\n",
    "    days= []\n",
    "    day_from = s\n",
    "    while (day_from) < e:\n",
    "        day_to = day_from + delta\n",
    "        days.append((day_from,day_to))\n",
    "        print '...for range %s to %s ...' % (day_from.strftime(\"%Y%m%d\"), day_to.strftime(\"%Y%m%d\"))\n",
    "        day_from += delta\n",
    "    return days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing getDates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...for range 20150101 to 20150130 ...\n",
      "...for range 20150130 to 20150228 ...\n",
      "...for range 20150228 to 20150329 ...\n",
      "...for range 20150329 to 20150427 ...\n",
      "...for range 20150427 to 20150526 ...\n",
      "...for range 20150526 to 20150624 ...\n",
      "...for range 20150624 to 20150723 ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2015, 1, 1), datetime.date(2015, 1, 30)),\n",
       " (datetime.date(2015, 1, 30), datetime.date(2015, 2, 28)),\n",
       " (datetime.date(2015, 2, 28), datetime.date(2015, 3, 29)),\n",
       " (datetime.date(2015, 3, 29), datetime.date(2015, 4, 27)),\n",
       " (datetime.date(2015, 4, 27), datetime.date(2015, 5, 26)),\n",
       " (datetime.date(2015, 5, 26), datetime.date(2015, 6, 24)),\n",
       " (datetime.date(2015, 6, 24), datetime.date(2015, 7, 23))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDates(20150101,20150630,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Function] saveUrlRDD\n",
    "\n",
    "This function, `saveUrlRDD` does the following:\n",
    "1. inputs as parameters a start_date and end_date\n",
    "1. generates a list of date ranges using `getDates` to feed to API request\n",
    "1. list of dates gets joined with list of stations\n",
    "1. for each station, an API request is sent per date range, using the latitude and longitude of the station\n",
    "1. data received from API is obtained as JSON, and written to file using Spark's `saveAsTextFile` command\n",
    "1. the text file generated is zipped to disk for backup\n",
    "1. `saveUrlRDD` returns a JSON RDD containing the all of the daily observations for all of the stations between the start and end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveUrlRDD(start_date, end_date, delta = None):\n",
    "    '''\n",
    "    start_date: date in the format YYYYMMDD. \n",
    "        E.g., 20100101\n",
    "    end_date: date in the format YYYYMMDD. \n",
    "        E.g., 20101231\n",
    "    delta: interval length between the two dates at which to retrieve day.\n",
    "        Default: end_date minus start_date in days, to a maximum of 30.\n",
    "        E.g., Set delta to 1 to retrieve data for every day\n",
    "        E.g., Set delta to 7 to retrieve data once every seven days\n",
    "    '''\n",
    "    print 'Starting...'\n",
    "    start = time.time()\n",
    "    \n",
    "    if delta == None:\n",
    "        s = datetime.date(int(str(start_date)[:4]),int(str(start_date)[4:6]),int(str(start_date)[6:]))\n",
    "        e = datetime.date(int(str(end_date)[:4]),int(str(end_date)[4:6]),int(str(end_date)[6:]))\n",
    "        delta = abs((e-s).days)\n",
    "        if delta > 30:\n",
    "            delta = 30\n",
    "\n",
    "    #Collect RDD list of dates to retrieve data\n",
    "    days = getDates(start_date, end_date, delta)\n",
    "    daysRDD = sc.parallelize(days)\n",
    "    \n",
    "    #Combine station list with dates to retrieve data on\n",
    "    stndateRDD=stRDD.cartesian(daysRDD)\n",
    "\n",
    "\n",
    "    print '...starting API GET request...'\n",
    "    \n",
    "    #Send API request based on station list and dates to retrieve data on\n",
    "    urlRDD = stndateRDD.map(lambda (s,d):requests.get('http://api.weather.com/v1/geocode/'+\n",
    "                                          str(s[1]) + '/' + str(s[2]) + \n",
    "                                          '/observations/historical.json?apiKey=' + \n",
    "                                          apiKey + '&units=m&startDate=' + \n",
    "                                          d[0].strftime(\"%Y%m%d\") + '&endDate=' + \n",
    "                                          d[1].strftime(\"%Y%m%d\"))\\\n",
    "                                          .json()) \n",
    "    #Set filename\n",
    "    filename= 'urlRDD'+'-Date-'+str(start_date)+'-'+str(end_date)\n",
    "    \n",
    "    if (os.path.isdir(\"/resources/\"+filename) & os.path.exists(\"/resources/\"+filename)):\n",
    "        print '...deleting existing folder...'\n",
    "        shutil.rmtree('/resources/'+filename) \n",
    "      \n",
    "    urlRDD.map(lambda x:json.dumps(x)).saveAsTextFile('/resources/'+filename)\n",
    "    \n",
    "    #zip up urlRDD file contents and save as backup\n",
    "    print '...zipping up the files...'\n",
    "    shutil.make_archive(filename, format=\"zip\", root_dir=(filename))\n",
    "    \n",
    "    #Check filesize of zip\n",
    "    filesize = os.stat('/resources/'+filename+'.zip').st_size/1000 #returns size of zip in kb\n",
    "    print '...zipped file is [%s kB] at \"/resources/%s.zip\" ...' % (filesize, filename)\n",
    "    \n",
    "    #Remove original text files after zipping\n",
    "    shutil.rmtree('/resources/'+filename) \n",
    "    \n",
    "    #check directory\n",
    "    print '...saveUrlRDD DONE. Elapsed time: [%s secs].' % str(round((time.time() - start),0))\n",
    "    print '...#Stations: %s...' % str(stRDD.count())\n",
    "    print '...#Records:  %s...' % str(stndateRDD.count())\n",
    "    \n",
    "    #optional for debugging:\n",
    "    return urlRDD.map(lambda x:json.dumps(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Function] preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, `preprocess`, does the following:\n",
    "1. Before the function is created, we create some user-defined functions to convert dates into human-readable Years, Months, Days, and Dates in Spark\n",
    "1. Inputs as its parameter the output of `saveUrlRDD`, a JSON RDD containing the all of the daily observations for all of the stations between the start and end date\n",
    "1. A Spark SQL Context is created which reads the dataRDD and convert it into a proper JSON RDD object that Spark SQL can use.\n",
    "1. The table is registered as a table in Spark using `registerTempTable`.\n",
    "1. The table is queried using `sqlContext.sql`.\n",
    "1. New columns for year, month, day, and date are created using the previously-defined UDFs.\n",
    "1. The RDD is cached to optimize processing speed.\n",
    "1. For each station and date, the MAX, MIN, and MEAN temperature is calculated, using `groupBy` and `agg`.\n",
    "1. The output, a relatively small dataframe, is exported as a Pandas dataframe.\n",
    "1. `preprocess` returns the Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Register user-defined functions as Spark UDFs \n",
    "udf_yearform = udf(lambda x:time.strftime('%Y', time.localtime(x)), pyspark.sql.types.StringType())\n",
    "udf_monthform = udf(lambda x:time.strftime('%m', time.localtime(x)), pyspark.sql.types.StringType())\n",
    "udf_dayform = udf(lambda x:time.strftime('%d', time.localtime(x)), pyspark.sql.types.StringType())\n",
    "udf_dateform = udf(lambda x:time.strftime('%Y-%m-%d', time.localtime(x)), pyspark.sql.types.StringType())\n",
    "\n",
    "def preprocess(dataRDD):\n",
    "    '''\n",
    "    Takes in a dataRDD containing a list of JSON RDDS, convert to SparkSQL dataframe.\n",
    "    Data is filtered, aggregated, and saved to file.\n",
    "    '''\n",
    "    start = time.time()\n",
    "    print '...Starting preprocess...'\n",
    "    \n",
    "    \n",
    "    sqlContext = HiveContext(sc)\n",
    "    Weather_sdf = sqlContext.jsonRDD(dataRDD)\n",
    "    \n",
    "    print Weather_sdf.printSchema()\n",
    "    \n",
    "    Weather_sdf.registerTempTable(\"weatherTable\")\n",
    "    Weather_sdf=sqlContext.sql(\"SELECT metadata.latitude AS LATITUDE,\\\n",
    "                 metadata.longitude AS LONGITUDE,\\\n",
    "                 observation.obs_name,\\\n",
    "                 observation.key,\\\n",
    "                 observation.valid_time_gmt,\\\n",
    "                 observation.temp\\\n",
    "                 FROM weatherTable LATERAL VIEW explode(observations) obstable as observation\")\n",
    "    \n",
    "    \n",
    "    # To add new columns\n",
    "    Weather_sdf=Weather_sdf.withColumn(\"obsYear\", udf_yearform(Weather_sdf['valid_time_gmt']))\n",
    "    Weather_sdf=Weather_sdf.withColumn(\"obsMonth\", udf_monthform(Weather_sdf['valid_time_gmt']))\n",
    "    Weather_sdf=Weather_sdf.withColumn(\"obsDay\", udf_dayform(Weather_sdf['valid_time_gmt']))\n",
    "    Weather_sdf=Weather_sdf.withColumn(\"obsDate\", udf_dateform(Weather_sdf['valid_time_gmt']))\n",
    "    \n",
    "    Weather_sdf.cache() #Very important. Speeds up entire pipeline.\n",
    "    \n",
    "    time1 = time.time()\n",
    "    \n",
    "    print 'PREPROCESS took [' + str(round(time1 - start,1)) + ' seconds] to run.'\n",
    "    \n",
    "    print Weather_sdf.show(3)\n",
    "    \n",
    "    print 'Processing using groupBy and agg...'\n",
    "    \n",
    "    # select data for plot\n",
    "    pdata = Weather_sdf.groupBy([\"obs_name\",\"obsDate\",\"LATITUDE\",\"LONGITUDE\"])\\\n",
    "    .agg(F.max(\"temp\"), F.min(\"temp\"), F.mean(\"temp\"))\n",
    "    \n",
    "    time2 = time.time()\n",
    "    \n",
    "    print 'GROUPBY/AGG took [' + str(round(time2 - time1,1)) + ' seconds] to run.'\n",
    "    \n",
    "    pdata.cache()\n",
    "    \n",
    "    print pdata.show(3)\n",
    "    \n",
    "    \n",
    "    # to write data\n",
    "    \n",
    "\n",
    "    print pdata.count()\n",
    "    pdf = pdata.toPandas()\n",
    "    \n",
    "    return pdf\n",
    "   \n",
    "    print '...finished saving to csv.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Function] save_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to save the result of `preprocess` to a .csv file for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_csv(x, start_date, end_date):\n",
    "    #x is the pandas dataframe\n",
    "    filename = 'USweather_' + str(start_date) + '-' + str(end_date)\n",
    "    print filename\n",
    "    if (os.path.isfile(\"/resources/\" + filename) & os.path.exists(\"/resources/\" + filename)):\n",
    "        os.remove('/resources/' + filename)\n",
    "    return x.to_csv('/resources/' + filename + \".csv\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Select start and end date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the range in dates that you want to retrieve data for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start_date and end_date should be in YYYYMMDD format\n",
    "\n",
    "start_date = 20140101\n",
    "end_date = 20140130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Run the scripts\n",
    "\n",
    "**Warning** this may several minutes to run. An API request is sent for each of the 97 stations, for the date range. If the date range exceeds 30 days, there will be an API request for each 30-day increment from the start until end date.\n",
    "\n",
    "**Estimated combined running time for `saveUrlRDD` and `preprocess`:**  \n",
    "With date range of\n",
    "- One month: approx. 2-3 minutes (e.g., 20140101 to 20140131)\n",
    "- One year: approx. 20 minutes (e.g., 20140101 to 20141231)\n",
    "\n",
    "\n",
    "Overview of what the script will do:\n",
    "1. **`saveUrlRDD`** calls on **`getDates`** to create an RDD of dates to send to API.  \n",
    "1. **`saveUrlRDD`** merges **`getDates`** data with list of weather stations in **stations.csv** \n",
    "1. **`saveUrlRDD`** sends an API request for each station, for each of the date intervals generated from **`getDates`**, saves the RDD result to textFile, and zips it up.  \n",
    "1. **`saveUrlRDD`** returns an RDD of the results in JSON format.  \n",
    "1. **`preprocess`** takes the JSON RDD output of **`saveUrlRDD`** and converts it to a Spark SQL Dataframe.\n",
    "1. The dataframe now consists of rows for each date for each station.\n",
    "1. Dataframe is aggregated by station and by date, and the follow information is returned:  \n",
    "  - **obs_name**: name of the weather station\n",
    "  - **obsDate**: the observation date in _YYYY-MM-DD_ format\n",
    "  - **LATITUDE**: latitude of the station\n",
    "  - **LONGITUDE**: longitude of the station\n",
    "  - **MAX(temp)**: maximum temperature\n",
    "  - **MIN(temp)**: minimum temperature\n",
    "  - **AVG(temp)**: average temperature\n",
    "1. The output of **`preprocess`** is a pandas dataframe, which becomes saved to disk as backup at _`/resources/processedYYYYMMDD-YYYYMMDD.csv`_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running `saveUrlRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running saveUrlRDD from 20140101 to 20140130\n",
      "Starting...\n",
      "...for range 20140101 to 20140129 ...\n",
      "...for range 20140129 to 20140226 ...\n",
      "...starting API GET request...\n",
      "...zipping up the files...\n",
      "...zipped file is [7014 kB] at \"/resources/urlRDD-Date-20140101-20140130.zip\" ...\n",
      "...saveUrlRDD DONE. Elapsed time: [173.0 secs].\n",
      "...#Stations: 97...\n",
      "...#Records:  194...\n",
      "TOTAL ELAPSED TIME : 173.155771971seconds\n"
     ]
    }
   ],
   "source": [
    "#Set start time\n",
    "start = time.time() \n",
    "\n",
    "\n",
    "#Run saveUrlRDD\n",
    "print 'Running saveUrlRDD from ' +str(start_date)+ ' to ' + str(end_date)\n",
    "result_saveUrlRDD = saveUrlRDD(start_date, end_date)\n",
    "\n",
    "\n",
    "#Set end time\n",
    "end = time.time()\n",
    "\n",
    "print 'TOTAL ELAPSED TIME : ' + str(end-start) + 'seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running `proprocess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Starting preprocess...\n",
      "root\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- expire_time_gmt: long (nullable = true)\n",
      " |    |-- language: string (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      " |    |-- status_code: long (nullable = true)\n",
      " |    |-- transaction_id: string (nullable = true)\n",
      " |    |-- units: string (nullable = true)\n",
      " |    |-- version: string (nullable = true)\n",
      " |-- observations: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- blunt_phrase: string (nullable = true)\n",
      " |    |    |-- class: string (nullable = true)\n",
      " |    |    |-- day_ind: string (nullable = true)\n",
      " |    |    |-- dewPt: long (nullable = true)\n",
      " |    |    |-- expire_time_gmt: long (nullable = true)\n",
      " |    |    |-- feels_like: long (nullable = true)\n",
      " |    |    |-- gust: long (nullable = true)\n",
      " |    |    |-- heat_index: long (nullable = true)\n",
      " |    |    |-- icon_extd: long (nullable = true)\n",
      " |    |    |-- key: string (nullable = true)\n",
      " |    |    |-- max_temp: long (nullable = true)\n",
      " |    |    |-- min_temp: long (nullable = true)\n",
      " |    |    |-- obs_id: string (nullable = true)\n",
      " |    |    |-- obs_name: string (nullable = true)\n",
      " |    |    |-- precip_hrly: double (nullable = true)\n",
      " |    |    |-- precip_total: string (nullable = true)\n",
      " |    |    |-- pressure: double (nullable = true)\n",
      " |    |    |-- pressure_desc: string (nullable = true)\n",
      " |    |    |-- pressure_tend: long (nullable = true)\n",
      " |    |    |-- qualifier: string (nullable = true)\n",
      " |    |    |-- qualifier_svrty: string (nullable = true)\n",
      " |    |    |-- rh: long (nullable = true)\n",
      " |    |    |-- snow_hrly: double (nullable = true)\n",
      " |    |    |-- temp: long (nullable = true)\n",
      " |    |    |-- terse_phrase: string (nullable = true)\n",
      " |    |    |-- uv_desc: string (nullable = true)\n",
      " |    |    |-- uv_index: long (nullable = true)\n",
      " |    |    |-- valid_time_gmt: long (nullable = true)\n",
      " |    |    |-- vis: double (nullable = true)\n",
      " |    |    |-- wc: long (nullable = true)\n",
      " |    |    |-- wdir: long (nullable = true)\n",
      " |    |    |-- wdir_cardinal: string (nullable = true)\n",
      " |    |    |-- wspd: long (nullable = true)\n",
      " |    |    |-- wx_icon: long (nullable = true)\n",
      " |    |    |-- wx_phrase: string (nullable = true)\n",
      "\n",
      "None\n",
      "PREPROCESS took [24.6 seconds] to run.\n",
      "+--------+---------+-----------+----+--------------+----+-------+--------+------+----------+\n",
      "|LATITUDE|LONGITUDE|   obs_name| key|valid_time_gmt|temp|obsYear|obsMonth|obsDay|   obsDate|\n",
      "+--------+---------+-----------+----+--------------+----+-------+--------+------+----------+\n",
      "|   39.83|  -104.65|Denver/Intl|KDEN|    1388559180|   4|   2014|      01|    01|2014-01-01|\n",
      "|   39.83|  -104.65|Denver/Intl|KDEN|    1388562780|   3|   2014|      01|    01|2014-01-01|\n",
      "|   39.83|  -104.65|Denver/Intl|KDEN|    1388566380|   2|   2014|      01|    01|2014-01-01|\n",
      "+--------+---------+-----------+----+--------------+----+-------+--------+------+----------+\n",
      "\n",
      "None\n",
      "Processing using groupBy and agg...\n",
      "GROUPBY/AGG took [92.7 seconds] to run.\n",
      "+-------------+----------+--------+---------+---------+---------+-------------------+\n",
      "|     obs_name|   obsDate|LATITUDE|LONGITUDE|MAX(temp)|MIN(temp)|          AVG(temp)|\n",
      "+-------------+----------+--------+---------+---------+---------+-------------------+\n",
      "|Niagara Falls|2014-02-04|    43.1|   -78.94|       -4|      -11|              -7.96|\n",
      "|        Fargo|2014-02-20|   46.92|   -96.81|        4|      -11|-3.6785714285714284|\n",
      "| Philadelphia|2014-02-15|   39.86|   -75.23|        3|        1| 1.6122448979591837|\n",
      "+-------------+----------+--------+---------+---------+---------+-------------------+\n",
      "\n",
      "None\n",
      "5567\n",
      "TOTAL ELAPSED TIME : 120.053508997seconds\n"
     ]
    }
   ],
   "source": [
    "#Set start time\n",
    "start = time.time() \n",
    "\n",
    "\n",
    "#Run preprocess on the result of saveUrlRDD\n",
    "pdf = preprocess(result_saveUrlRDD) \n",
    "\n",
    "\n",
    "#Set end time\n",
    "end = time.time()\n",
    "\n",
    "print 'TOTAL ELAPSED TIME : ' + str(end-start) + 'seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running `save_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USweather_20140101-20140130\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs_name</th>\n",
       "      <th>obsDate</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>MAX(temp)</th>\n",
       "      <th>MIN(temp)</th>\n",
       "      <th>AVG(temp)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> Niagara Falls</td>\n",
       "      <td> 2014-02-04</td>\n",
       "      <td> 43.10</td>\n",
       "      <td>-78.94</td>\n",
       "      <td>-4</td>\n",
       "      <td>-11</td>\n",
       "      <td> -7.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>         Fargo</td>\n",
       "      <td> 2014-02-20</td>\n",
       "      <td> 46.92</td>\n",
       "      <td>-96.81</td>\n",
       "      <td> 4</td>\n",
       "      <td>-11</td>\n",
       "      <td> -3.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>  Philadelphia</td>\n",
       "      <td> 2014-02-15</td>\n",
       "      <td> 39.86</td>\n",
       "      <td>-75.23</td>\n",
       "      <td> 3</td>\n",
       "      <td>  1</td>\n",
       "      <td>  1.612245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>       Houlton</td>\n",
       "      <td> 2014-02-26</td>\n",
       "      <td> 46.12</td>\n",
       "      <td>-67.79</td>\n",
       "      <td>-7</td>\n",
       "      <td>-24</td>\n",
       "      <td>-13.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>        Boston</td>\n",
       "      <td> 2014-02-09</td>\n",
       "      <td> 42.36</td>\n",
       "      <td>-71.01</td>\n",
       "      <td>-1</td>\n",
       "      <td> -6</td>\n",
       "      <td> -3.437500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        obs_name     obsDate  LATITUDE  LONGITUDE  MAX(temp)  MIN(temp)  \\\n",
       "0  Niagara Falls  2014-02-04     43.10     -78.94         -4        -11   \n",
       "1          Fargo  2014-02-20     46.92     -96.81          4        -11   \n",
       "2   Philadelphia  2014-02-15     39.86     -75.23          3          1   \n",
       "3        Houlton  2014-02-26     46.12     -67.79         -7        -24   \n",
       "4         Boston  2014-02-09     42.36     -71.01         -1         -6   \n",
       "\n",
       "   AVG(temp)  \n",
       "0  -7.960000  \n",
       "1  -3.678571  \n",
       "2   1.612245  \n",
       "3 -13.080000  \n",
       "4  -3.437500  \n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save to CSV file\n",
    "save_csv(pdf, start_date, end_date)\n",
    "\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did you notice some files being created during the scripts?\n",
    "1. Initiating the Spark context automatically generates some files, such as 'derby.log'. These files can be safely ignored.\n",
    "2. A folder containing a number of 'part' files are created from Spark's `.saveAsTextFile` command, which contains an RDD of all of the weather station observations. Once this folder becomes zipped in the UDF `saveUrlRDD`, we have chosen to delete the folder containing part files to keep the disk clean. \n",
    "3. A `.csv` file is created. This file becomes the input for the Data Visualization notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Quality-check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0       obs_name     obsDate  LATITUDE  LONGITUDE  MAX(temp)  \\\n",
      "0           0  Niagara Falls  2014-02-04     43.10     -78.94         -4   \n",
      "1           1          Fargo  2014-02-20     46.92     -96.81          4   \n",
      "2           2   Philadelphia  2014-02-15     39.86     -75.23          3   \n",
      "3           3        Houlton  2014-02-26     46.12     -67.79         -7   \n",
      "4           4         Boston  2014-02-09     42.36     -71.01         -1   \n",
      "\n",
      "   MIN(temp)  AVG(temp)  \n",
      "0        -11  -7.960000  \n",
      "1        -11  -3.678571  \n",
      "2          1   1.612245  \n",
      "3        -24 -13.080000  \n",
      "4         -6  -3.437500  \n",
      "\n",
      "[5 rows x 8 columns]\n",
      "      Unnamed: 0 obs_name     obsDate  LATITUDE  LONGITUDE  MAX(temp)  \\\n",
      "92            92  Phoenix  2014-01-03     33.42       -112         23   \n",
      "296          296  Phoenix  2014-01-14     33.42       -112         23   \n",
      "354          354  Phoenix  2014-01-30     33.42       -112         24   \n",
      "381          381  Phoenix  2014-02-10     33.42       -112         26   \n",
      "458          458  Phoenix  2014-01-09     33.42       -112         19   \n",
      "527          527  Phoenix  2014-01-25     33.42       -112         23   \n",
      "551          551  Phoenix  2014-02-05     33.42       -112         18   \n",
      "607          607  Phoenix  2014-02-21     33.42       -112         24   \n",
      "791          791  Phoenix  2014-02-16     33.42       -112         29   \n",
      "950          950  Phoenix  2014-01-02     33.42       -112         24   \n",
      "1000        1000  Phoenix  2014-02-27     33.42       -112         26   \n",
      "1155        1155  Phoenix  2014-01-13     33.42       -112         21   \n",
      "1302        1302  Phoenix  2014-01-08     33.42       -112         20   \n",
      "1369        1369  Phoenix  2014-01-24     33.42       -112         21   \n",
      "1390        1390  Phoenix  2014-02-04     33.42       -112         17   \n",
      "1465        1465  Phoenix  2014-02-20     33.42       -112         26   \n",
      "1551        1551  Phoenix  2014-01-19     33.42       -112         24   \n",
      "1641        1641  Phoenix  2014-02-15     33.42       -112         30   \n",
      "1797        1797  Phoenix  2014-01-01     33.42       -112         21   \n",
      "1860        1860  Phoenix  2014-02-26     33.42       -112         27   \n",
      "2034        2034  Phoenix  2014-01-12     33.42       -112         20   \n",
      "2184        2184  Phoenix  2014-01-07     33.42       -112         20   \n",
      "2257        2257  Phoenix  2014-01-23     33.42       -112         23   \n",
      "2268        2268  Phoenix  2014-02-03     33.42       -112         17   \n",
      "2422        2422  Phoenix  2014-01-18     33.42       -112         23   \n",
      "2497        2497  Phoenix  2014-02-14     33.42       -112         29   \n",
      "2647        2647  Phoenix  2014-01-29     33.42       -112         23   \n",
      "2679        2679  Phoenix  2014-02-09     33.42       -112         24   \n",
      "2739        2739  Phoenix  2014-02-25     33.42       -112         27   \n",
      "2900        2900  Phoenix  2014-01-11     33.42       -112         21   \n",
      "3053        3053  Phoenix  2014-01-06     33.42       -112         19   \n",
      "3109        3109  Phoenix  2014-01-22     33.42       -112         25   \n",
      "3151        3151  Phoenix  2014-02-02     33.42       -112         17   \n",
      "3273        3273  Phoenix  2014-01-17     33.42       -112         26   \n",
      "3370        3370  Phoenix  2014-02-13     33.42       -112         27   \n",
      "3500        3500  Phoenix  2014-01-28     33.42       -112         22   \n",
      "3543        3543  Phoenix  2014-02-08     33.42       -112         21   \n",
      "3604        3604  Phoenix  2014-02-24     33.42       -112         28   \n",
      "3753        3753  Phoenix  2014-02-19     33.42       -112         28   \n",
      "3766        3766  Phoenix  2014-01-10     33.42       -112         19   \n",
      "3923        3923  Phoenix  2014-01-05     33.42       -112         18   \n",
      "3979        3979  Phoenix  2014-01-21     33.42       -112         27   \n",
      "4018        4018  Phoenix  2014-02-01     33.42       -112         19   \n",
      "4142        4142  Phoenix  2014-01-16     33.42       -112         23   \n",
      "4226        4226  Phoenix  2014-02-12     33.42       -112         25   \n",
      "4369        4369  Phoenix  2014-01-27     33.42       -112         24   \n",
      "4410        4410  Phoenix  2014-02-07     33.42       -112         19   \n",
      "4466        4466  Phoenix  2014-02-23     33.42       -112         27   \n",
      "4638        4638  Phoenix  2014-02-18     33.42       -112         29   \n",
      "4799        4799  Phoenix  2014-01-04     33.42       -112         20   \n",
      "4854        4854  Phoenix  2014-01-20     33.42       -112         24   \n",
      "5000        5000  Phoenix  2014-01-15     33.42       -112         24   \n",
      "5055        5055  Phoenix  2014-01-31     33.42       -112         24   \n",
      "5100        5100  Phoenix  2014-02-11     33.42       -112         26   \n",
      "5235        5235  Phoenix  2014-01-26     33.42       -112         23   \n",
      "5271        5271  Phoenix  2014-02-06     33.42       -112         17   \n",
      "5316        5316  Phoenix  2014-02-22     33.42       -112         26   \n",
      "5489        5489  Phoenix  2014-02-17     33.42       -112         29   \n",
      "\n",
      "      MIN(temp)  AVG(temp)  \n",
      "92            9  15.958333  \n",
      "296           6  13.125000  \n",
      "354          11  17.333333  \n",
      "381          12  17.750000  \n",
      "458           7  12.086957  \n",
      "527          16  18.875000  \n",
      "551          11  13.958333  \n",
      "607          12  17.875000  \n",
      "791          13  21.041667  \n",
      "950           7  14.416667  \n",
      "1000         21  23.333333  \n",
      "1155          6  13.458333  \n",
      "1302          7  13.750000  \n",
      "1369         14  17.708333  \n",
      "1390          5  10.833333  \n",
      "1465         12  19.250000  \n",
      "1551          9  15.875000  \n",
      "1641         12  20.833333  \n",
      "1797          7  12.888889  \n",
      "1860         13  20.041667  \n",
      "2034          8  13.125000  \n",
      "2184          7  13.000000  \n",
      "2257          9  16.375000  \n",
      "2268         10  13.666667  \n",
      "2422          9  15.958333  \n",
      "2497         13  20.250000  \n",
      "2647          9  15.190476  \n",
      "2679         11  16.750000  \n",
      "2739         15  20.250000  \n",
      "2900          7  13.083333  \n",
      "3053          5  12.000000  \n",
      "3109         13  18.208333  \n",
      "3151          6  12.041667  \n",
      "3273          7  15.875000  \n",
      "3370         11  18.958333  \n",
      "3500          9  15.916667  \n",
      "3543         10  15.291667  \n",
      "3604         13  20.000000  \n",
      "3753         15  21.208333  \n",
      "3766          6  12.041667  \n",
      "3923          7  12.208333  \n",
      "3979          9  16.750000  \n",
      "4018         12  15.041667  \n",
      "4142          7  15.041667  \n",
      "4226         11  18.000000  \n",
      "4369         14  17.625000  \n",
      "4410         10  15.000000  \n",
      "4466         13  19.416667  \n",
      "4638         14  21.708333  \n",
      "4799          9  14.000000  \n",
      "4854          9  15.583333  \n",
      "5000          9  15.958333  \n",
      "5055         13  17.833333  \n",
      "5100         12  18.416667  \n",
      "5235         11  16.583333  \n",
      "5271          9  12.500000  \n",
      "5316         11  17.875000  \n",
      "5489         14  21.333333  \n",
      "\n",
      "[58 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "check = pd.read_csv('/resources/USweather_'+str(start_date)+'-'+str(end_date)+'.csv')\n",
    "print check.head(5)\n",
    "print check[check['obs_name'] == 'Phoenix']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Check to make sure the output .csv file exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the file under 'Recent Data' in the sidebar. Here is another way to confirm its existence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/resources/USweather_20140101-20140130.csv\r\n"
     ]
    }
   ],
   "source": [
    "#Search for text files that start with 'USweather_'\n",
    "!ls /resources/USweather_*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contact the Notebook Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **[Polong Lin](https://ca.linkedin.com/in/polonglin), Data Scientist, IBM.** polong[at]ca.ibm.com\n",
    "1. **[Saeed Aghabozorgi](https://ca.linkedin.com/in/saeedaghabozorgi), Data Scientist, IBM.** saeed[at]ca.ibm.com  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
